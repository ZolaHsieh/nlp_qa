{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lR-SudO1ACip"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XDiJ7moI__s8"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets & pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data: Squad\n",
        "- five columns: `'id', 'title', 'context', 'question', 'answers'`"
      ],
      "metadata": {
        "id": "lR-SudO1ACip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"squad\")\n",
        "raw_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxLa3gdkAHeO",
        "outputId": "844a8397-db0c-4e31-d837-33c4b3cca3ae"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['id'][0], raw_datasets['train']['title'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERhsRpgdAIj3",
        "outputId": "44926778-6a13-4fae-93bd-97a597b6774b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('5733be284776f41900661182', 'University_of_Notre_Dame')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['question'][0], raw_datasets['train']['answers'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRRv2sG_Ack0",
        "outputId": "bfc2bc15-eb12-4f1f-e7fa-b9ed4abf1ef6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['context'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "RUJd6Ql8RAbb",
        "outputId": "bcef4f2b-2b7f-4efb-c855-111d8aa4cd9a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 有可能有多個ans\n",
        "raw_datasets[\"validation\"][2][\"answers\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WI47tDxAzwK",
        "outputId": "4b8d15ef-4459-445b-8d4e-c145cbb06476"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['Santa Clara, California',\n",
              "  \"Levi's Stadium\",\n",
              "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"],\n",
              " 'answer_start': [403, 355, 355]}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer: 將word分割成片段並轉換為有意義的表示法"
      ],
      "metadata": {
        "id": "sjpa5UV0A8MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\" # \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "_UByEWf4BDAl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = raw_datasets[\"train\"][1][\"context\"]\n",
        "question = raw_datasets[\"train\"][1][\"question\"]\n",
        "\n",
        "inputs = tokenizer(question, context)\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icXVbCUHBC-Z",
        "outputId": "1f9b901f-d022-47eb-c6e2-407f210cedb8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### decode：將轉換後的id mapping 回 word"
      ],
      "metadata": {
        "id": "vcrF9uMWEkBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "y5H6bvqhBC7w",
        "outputId": "ef93f903-7e5c-4256-affa-80b521d2ffd3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 因為context可能會超過model可以input的size限制，因此在tokenize的時候需要將context切開"
      ],
      "metadata": {
        "id": "8fTdhsQyFnkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text = question,\n",
        "                   text_pair = context,\n",
        "                   max_length=100, # the maximum length of context\n",
        "                   truncation=\"only_second\", # 不能truncate question\n",
        "                   stride=50, # move forward 50 (overlap 50 chars)\n",
        "                   return_overflowing_tokens=True)\n",
        "\n",
        "inputs.keys(), len(inputs.input_ids) #被切成4 份"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-wOebD1BC5R",
        "outputId": "8c7becbc-ac0b-4aa3-9b9c-dcec93b6a718"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping']), 4)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "格式：[CLS] question [SEP] context"
      ],
      "metadata": {
        "id": "36RFHqbDFsZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, input_ids in enumerate(inputs[\"input_ids\"]):\n",
        "    print(id, len(input_ids), tokenizer.decode(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Rj09AiB0aa",
        "outputId": "853c79ae-9e9a-4b96-992c-2c58f9601ceb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 100 [CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the G [SEP]\n",
            "1 100 [CLS] What is in front of the Notre Dame Main Building? [SEP] facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernade [SEP]\n",
            "2 100 [CLS] What is in front of the Notre Dame Main Building? [SEP] of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern [SEP]\n",
            "3 69 [CLS] What is in front of the Notre Dame Main Building? [SEP]rdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " return_overflowing_tokens=True 產生，內容為input 跟切分後資料的id mapping關係"
      ],
      "metadata": {
        "id": "55wIQZJoFv9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['overflow_to_sample_mapping']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Ijun34Cew0",
        "outputId": "3c688cff-7964-4727-c814-b75e53031903"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 三個例子，各自切成四份，可以用overflow_to_sample_mapping 對照回原資料的id\n",
        "inputs = tokenizer(text = raw_datasets[\"train\"][:3][\"question\"],\n",
        "                   text_pair = raw_datasets[\"train\"][:3][\"context\"],\n",
        "                   max_length=100,\n",
        "                   truncation=\"only_second\",\n",
        "                   stride=50,\n",
        "                   return_overflowing_tokens=True)\n",
        "\n",
        "inputs['overflow_to_sample_mapping']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkb_5sZyDRm-",
        "outputId": "b01ce7a6-0eb7-4810-9e15-0916e5605dc4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "return_offsets_mapping=True：tells us the location of each token"
      ],
      "metadata": {
        "id": "r6cxZszqGKN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text = question,\n",
        "                   text_pair = context,\n",
        "                   max_length=100, # the maximum length of context\n",
        "                   truncation=\"only_second\", # 不能truncate question\n",
        "                   stride=50, # move forward 50 (overlap 50 chars)\n",
        "                   return_overflowing_tokens=True,\n",
        "                   return_offsets_mapping=True)\n",
        "inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSjBYQB2D7TD",
        "outputId": "2608e094-ed93-48c9-f435-b606ec3d3e37"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['offset_mapping'] ##每個token在字串中的位置"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzlKEDxaEZKD",
        "outputId": "02282717-f28b-4058-a298-d10aad2778f3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 0),\n",
              "  (0, 4),\n",
              "  (5, 7),\n",
              "  (8, 10),\n",
              "  (11, 16),\n",
              "  (17, 19),\n",
              "  (20, 23),\n",
              "  (24, 29),\n",
              "  (30, 34),\n",
              "  (35, 39),\n",
              "  (40, 48),\n",
              "  (48, 49),\n",
              "  (0, 0),\n",
              "  (0, 13),\n",
              "  (13, 15),\n",
              "  (15, 16),\n",
              "  (17, 20),\n",
              "  (21, 27),\n",
              "  (28, 31),\n",
              "  (32, 33),\n",
              "  (34, 42),\n",
              "  (43, 52),\n",
              "  (52, 53),\n",
              "  (54, 56),\n",
              "  (56, 58),\n",
              "  (59, 62),\n",
              "  (63, 67),\n",
              "  (68, 76),\n",
              "  (76, 77),\n",
              "  (77, 78),\n",
              "  (79, 83),\n",
              "  (84, 88),\n",
              "  (89, 91),\n",
              "  (92, 93),\n",
              "  (94, 100),\n",
              "  (101, 107),\n",
              "  (108, 110),\n",
              "  (111, 114),\n",
              "  (115, 121),\n",
              "  (122, 126),\n",
              "  (126, 127),\n",
              "  (128, 139),\n",
              "  (140, 142),\n",
              "  (143, 148),\n",
              "  (149, 151),\n",
              "  (152, 155),\n",
              "  (156, 160),\n",
              "  (161, 169),\n",
              "  (170, 173),\n",
              "  (174, 180),\n",
              "  (181, 183),\n",
              "  (183, 184),\n",
              "  (185, 187),\n",
              "  (188, 189),\n",
              "  (190, 196),\n",
              "  (197, 203),\n",
              "  (204, 206),\n",
              "  (207, 213),\n",
              "  (214, 218),\n",
              "  (219, 223),\n",
              "  (224, 226),\n",
              "  (226, 229),\n",
              "  (229, 232),\n",
              "  (233, 237),\n",
              "  (238, 241),\n",
              "  (242, 248),\n",
              "  (249, 250),\n",
              "  (250, 251),\n",
              "  (251, 254),\n",
              "  (254, 256),\n",
              "  (257, 259),\n",
              "  (260, 262),\n",
              "  (263, 264),\n",
              "  (264, 265),\n",
              "  (265, 268),\n",
              "  (268, 269),\n",
              "  (269, 270),\n",
              "  (271, 275),\n",
              "  (276, 278),\n",
              "  (279, 282),\n",
              "  (283, 287),\n",
              "  (288, 296),\n",
              "  (297, 299),\n",
              "  (300, 303),\n",
              "  (304, 312),\n",
              "  (313, 315),\n",
              "  (316, 319),\n",
              "  (320, 326),\n",
              "  (327, 332),\n",
              "  (332, 333),\n",
              "  (334, 345),\n",
              "  (346, 352),\n",
              "  (353, 356),\n",
              "  (357, 358),\n",
              "  (358, 361),\n",
              "  (361, 365),\n",
              "  (366, 368),\n",
              "  (369, 372),\n",
              "  (373, 374),\n",
              "  (0, 0)],\n",
              " [(0, 0),\n",
              "  (0, 4),\n",
              "  (5, 7),\n",
              "  (8, 10),\n",
              "  (11, 16),\n",
              "  (17, 19),\n",
              "  (20, 23),\n",
              "  (24, 29),\n",
              "  (30, 34),\n",
              "  (35, 39),\n",
              "  (40, 48),\n",
              "  (48, 49),\n",
              "  (0, 0),\n",
              "  (174, 180),\n",
              "  (181, 183),\n",
              "  (183, 184),\n",
              "  (185, 187),\n",
              "  (188, 189),\n",
              "  (190, 196),\n",
              "  (197, 203),\n",
              "  (204, 206),\n",
              "  (207, 213),\n",
              "  (214, 218),\n",
              "  (219, 223),\n",
              "  (224, 226),\n",
              "  (226, 229),\n",
              "  (229, 232),\n",
              "  (233, 237),\n",
              "  (238, 241),\n",
              "  (242, 248),\n",
              "  (249, 250),\n",
              "  (250, 251),\n",
              "  (251, 254),\n",
              "  (254, 256),\n",
              "  (257, 259),\n",
              "  (260, 262),\n",
              "  (263, 264),\n",
              "  (264, 265),\n",
              "  (265, 268),\n",
              "  (268, 269),\n",
              "  (269, 270),\n",
              "  (271, 275),\n",
              "  (276, 278),\n",
              "  (279, 282),\n",
              "  (283, 287),\n",
              "  (288, 296),\n",
              "  (297, 299),\n",
              "  (300, 303),\n",
              "  (304, 312),\n",
              "  (313, 315),\n",
              "  (316, 319),\n",
              "  (320, 326),\n",
              "  (327, 332),\n",
              "  (332, 333),\n",
              "  (334, 345),\n",
              "  (346, 352),\n",
              "  (353, 356),\n",
              "  (357, 358),\n",
              "  (358, 361),\n",
              "  (361, 365),\n",
              "  (366, 368),\n",
              "  (369, 372),\n",
              "  (373, 374),\n",
              "  (374, 377),\n",
              "  (377, 379),\n",
              "  (379, 380),\n",
              "  (381, 382),\n",
              "  (383, 389),\n",
              "  (390, 395),\n",
              "  (396, 398),\n",
              "  (399, 405),\n",
              "  (406, 409),\n",
              "  (410, 420),\n",
              "  (420, 421),\n",
              "  (422, 424),\n",
              "  (425, 427),\n",
              "  (428, 429),\n",
              "  (430, 437),\n",
              "  (438, 440),\n",
              "  (441, 444),\n",
              "  (445, 446),\n",
              "  (446, 449),\n",
              "  (449, 451),\n",
              "  (452, 454),\n",
              "  (455, 458),\n",
              "  (458, 462),\n",
              "  (462, 463),\n",
              "  (464, 470),\n",
              "  (471, 476),\n",
              "  (477, 480),\n",
              "  (481, 487),\n",
              "  (488, 492),\n",
              "  (493, 500),\n",
              "  (500, 502),\n",
              "  (503, 511),\n",
              "  (512, 514),\n",
              "  (515, 520),\n",
              "  (521, 525),\n",
              "  (525, 528),\n",
              "  (0, 0)],\n",
              " [(0, 0),\n",
              "  (0, 4),\n",
              "  (5, 7),\n",
              "  (8, 10),\n",
              "  (11, 16),\n",
              "  (17, 19),\n",
              "  (20, 23),\n",
              "  (24, 29),\n",
              "  (30, 34),\n",
              "  (35, 39),\n",
              "  (40, 48),\n",
              "  (48, 49),\n",
              "  (0, 0),\n",
              "  (313, 315),\n",
              "  (316, 319),\n",
              "  (320, 326),\n",
              "  (327, 332),\n",
              "  (332, 333),\n",
              "  (334, 345),\n",
              "  (346, 352),\n",
              "  (353, 356),\n",
              "  (357, 358),\n",
              "  (358, 361),\n",
              "  (361, 365),\n",
              "  (366, 368),\n",
              "  (369, 372),\n",
              "  (373, 374),\n",
              "  (374, 377),\n",
              "  (377, 379),\n",
              "  (379, 380),\n",
              "  (381, 382),\n",
              "  (383, 389),\n",
              "  (390, 395),\n",
              "  (396, 398),\n",
              "  (399, 405),\n",
              "  (406, 409),\n",
              "  (410, 420),\n",
              "  (420, 421),\n",
              "  (422, 424),\n",
              "  (425, 427),\n",
              "  (428, 429),\n",
              "  (430, 437),\n",
              "  (438, 440),\n",
              "  (441, 444),\n",
              "  (445, 446),\n",
              "  (446, 449),\n",
              "  (449, 451),\n",
              "  (452, 454),\n",
              "  (455, 458),\n",
              "  (458, 462),\n",
              "  (462, 463),\n",
              "  (464, 470),\n",
              "  (471, 476),\n",
              "  (477, 480),\n",
              "  (481, 487),\n",
              "  (488, 492),\n",
              "  (493, 500),\n",
              "  (500, 502),\n",
              "  (503, 511),\n",
              "  (512, 514),\n",
              "  (515, 520),\n",
              "  (521, 525),\n",
              "  (525, 528),\n",
              "  (528, 531),\n",
              "  (532, 534),\n",
              "  (534, 537),\n",
              "  (537, 541),\n",
              "  (542, 544),\n",
              "  (545, 549),\n",
              "  (549, 550),\n",
              "  (551, 553),\n",
              "  (554, 557),\n",
              "  (558, 561),\n",
              "  (562, 564),\n",
              "  (565, 568),\n",
              "  (569, 573),\n",
              "  (574, 579),\n",
              "  (580, 581),\n",
              "  (581, 584),\n",
              "  (585, 587),\n",
              "  (588, 589),\n",
              "  (590, 596),\n",
              "  (597, 601),\n",
              "  (602, 606),\n",
              "  (607, 615),\n",
              "  (616, 623),\n",
              "  (624, 625),\n",
              "  (626, 633),\n",
              "  (634, 637),\n",
              "  (638, 641),\n",
              "  (642, 646),\n",
              "  (647, 651),\n",
              "  (651, 652),\n",
              "  (652, 653),\n",
              "  (654, 656),\n",
              "  (657, 658),\n",
              "  (659, 665),\n",
              "  (665, 666),\n",
              "  (667, 673),\n",
              "  (0, 0)],\n",
              " [(0, 0),\n",
              "  (0, 4),\n",
              "  (5, 7),\n",
              "  (8, 10),\n",
              "  (11, 16),\n",
              "  (17, 19),\n",
              "  (20, 23),\n",
              "  (24, 29),\n",
              "  (30, 34),\n",
              "  (35, 39),\n",
              "  (40, 48),\n",
              "  (48, 49),\n",
              "  (0, 0),\n",
              "  (458, 462),\n",
              "  (462, 463),\n",
              "  (464, 470),\n",
              "  (471, 476),\n",
              "  (477, 480),\n",
              "  (481, 487),\n",
              "  (488, 492),\n",
              "  (493, 500),\n",
              "  (500, 502),\n",
              "  (503, 511),\n",
              "  (512, 514),\n",
              "  (515, 520),\n",
              "  (521, 525),\n",
              "  (525, 528),\n",
              "  (528, 531),\n",
              "  (532, 534),\n",
              "  (534, 537),\n",
              "  (537, 541),\n",
              "  (542, 544),\n",
              "  (545, 549),\n",
              "  (549, 550),\n",
              "  (551, 553),\n",
              "  (554, 557),\n",
              "  (558, 561),\n",
              "  (562, 564),\n",
              "  (565, 568),\n",
              "  (569, 573),\n",
              "  (574, 579),\n",
              "  (580, 581),\n",
              "  (581, 584),\n",
              "  (585, 587),\n",
              "  (588, 589),\n",
              "  (590, 596),\n",
              "  (597, 601),\n",
              "  (602, 606),\n",
              "  (607, 615),\n",
              "  (616, 623),\n",
              "  (624, 625),\n",
              "  (626, 633),\n",
              "  (634, 637),\n",
              "  (638, 641),\n",
              "  (642, 646),\n",
              "  (647, 651),\n",
              "  (651, 652),\n",
              "  (652, 653),\n",
              "  (654, 656),\n",
              "  (657, 658),\n",
              "  (659, 665),\n",
              "  (665, 666),\n",
              "  (667, 673),\n",
              "  (674, 679),\n",
              "  (680, 686),\n",
              "  (687, 689),\n",
              "  (690, 694),\n",
              "  (694, 695),\n",
              "  (0, 0)]]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(inputs['offset_mapping']), len(inputs['offset_mapping'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha4ARhenGUel",
        "outputId": "0e0339d2-c8af-4182-c63f-2f565bb1c5a1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Alignment: 因為將context切割，answer在切割後的位置會不同，也有可能不會出現\n",
        "-> 將ans位置對應到每筆切割後的位置作為target"
      ],
      "metadata": {
        "id": "kUYrPkd8IwjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = raw_datasets[\"train\"][1][\"answers\"]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-eK8eqLGjwU",
        "outputId": "5b2f865d-eab8-470b-c017-482b56934f75"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequence_ids：tokenizer output 來源為function的第幾個input string"
      ],
      "metadata": {
        "id": "AnY0_xB4J4EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.sequence_ids(0)) #tokenizer的輸入來源，0表示第一個輸入(也就是question), 1表示第二個輸入(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGJCLLIbJ1ux",
        "outputId": "9859d86a-31c2-425e-ca17-09b390ac5f90"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the start and end of the context (the first and last '1')\n",
        "sequence_ids = inputs.sequence_ids(0)\n",
        "\n",
        "ctx_start = sequence_ids.index(1) # first occurrence\n",
        "ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1 # last occurrence\n",
        "\n",
        "ctx_start, ctx_end #切分後的context內容在 [13, 98](前面為question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eswx5UWNIv7y",
        "outputId": "c0802b33-8d34-4008-ec43-17e841fc7ddd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 98)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ans 在原文中的位置\n",
        "ans_start_char = answer['answer_start'][0]\n",
        "ans_end_char = ans_start_char + len(answer['text'][0])\n",
        "ans_start_char, ans_end_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCh1vvy8KJpC",
        "outputId": "c46c34ea-5490-4c97-fe22-aa19bc18b6c2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(188, 213)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 切割後的第1筆資料當作例子\n",
        "offset = inputs['offset_mapping'][0]\n",
        "start_idx = 0\n",
        "end_idx = 0\n",
        "\n",
        "if (ans_start_char >= offset[ctx_start][0]) and (ans_end_char <= offset[ctx_end][1]): # ans 位置在切割後的context內\n",
        "    i = ctx_start\n",
        "    for start_end_char in offset[ctx_start:]:\n",
        "        start, end = start_end_char\n",
        "        if start == ans_start_char:\n",
        "            start_idx = i\n",
        "\n",
        "        if end == ans_end_char:\n",
        "            end_idx = i\n",
        "            break\n",
        "\n",
        "        i += 1\n",
        "\n",
        "else:\n",
        "     print(\"target is (0, 0)\")\n",
        "     # nothing else to do\n",
        "\n",
        "start_idx, end_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT7Q9JTcJVzy",
        "outputId": "cace33f7-4e10-420c-a278-b466c4fd190c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53, 57)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = inputs['input_ids'][0]\n",
        "input_ids[start_idx : end_idx + 1], tokenizer.decode(input_ids[start_idx : end_idx + 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-446wNi_LyIi",
        "outputId": "754005a1-26e2-47b6-d5e7-06d3c5b9ae09"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([170, 7335, 5921, 1104, 4028], 'a copper statue of Christ')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_answer_token_idx(ctx_start, ctx_end,\n",
        "                          ans_start_char, ans_end_char,\n",
        "                          offset):\n",
        "    start_idx = 0\n",
        "    end_idx = 0\n",
        "\n",
        "    if (ans_start_char >= offset[ctx_start][0]) and (ans_end_char <= offset[ctx_end][1]): # ans 位置在切割後的context內\n",
        "        i = ctx_start\n",
        "        for start_end_char in offset[ctx_start:]:\n",
        "            start, end = start_end_char\n",
        "            if start == ans_start_char:\n",
        "                start_idx = i\n",
        "\n",
        "            if end == ans_end_char:\n",
        "                end_idx = i\n",
        "                break\n",
        "\n",
        "            i += 1\n",
        "    return start_idx, end_idx"
      ],
      "metadata": {
        "id": "Ofl6iiKdMp3v"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_idxs = []\n",
        "end_idxs = []\n",
        "\n",
        "## ans 在原文中的位置\n",
        "ans_start_char = answer['answer_start'][0]\n",
        "ans_end_char = ans_start_char + len(answer['text'][0])\n",
        "\n",
        "for i, offset in enumerate(inputs['offset_mapping']):\n",
        "\n",
        "    #找到切割後的句子arr中context 位置\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "    # print(len(offset), len(sequence_ids))\n",
        "\n",
        "    ctx_start = sequence_ids.index(1) # first occurrence\n",
        "    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1 # last occurrence\n",
        "\n",
        "\n",
        "    new_s, new_e = find_answer_token_idx(ctx_start, ctx_end,\n",
        "                                         ans_start_char, ans_end_char,\n",
        "                                         offset)\n",
        "    start_idxs.append(new_s)\n",
        "    end_idxs.append(new_e)\n",
        "\n",
        "start_idxs, end_idxs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CuGky3ANMeJ",
        "outputId": "3cf75188-c9ce-4b8b-da28-46c45c03e6ef"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([53, 17, 0, 0], [57, 21, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(start_idxs)):\n",
        "    input_ids = inputs['input_ids'][i]\n",
        "    start_idx, end_idx = start_idxs[i], end_idxs[i]\n",
        "\n",
        "    print(input_ids[start_idx : end_idx + 1],\n",
        "          tokenizer.decode(input_ids[start_idx : end_idx + 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMsmnzNwNq3z",
        "outputId": "8b7d569f-a5cf-4ab8-caa5-f8fae4089e43"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170, 7335, 5921, 1104, 4028] a copper statue of Christ\n",
            "[170, 7335, 5921, 1104, 4028] a copper statue of Christ\n",
            "[101] [CLS]\n",
            "[101] [CLS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer func. for Training set & Val set"
      ],
      "metadata": {
        "id": "f8EEX7CqOB09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google used 384 for SQuAD\n",
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "def tokenize_fn_train(batch):\n",
        "\n",
        "    questions = [q.strip() for q in batch['question']]\n",
        "    inputs = tokenizer(text = questions,\n",
        "                       text_pair = batch['context'],\n",
        "                       max_length=max_length, # the maximum length of context\n",
        "                       truncation='only_second', # 不能truncate question\n",
        "                       stride=stride, # move forward 50 (overlap 50 chars)\n",
        "                       return_overflowing_tokens=True,\n",
        "                       return_offsets_mapping=True,\n",
        "                       padding='max_length')\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = batch['answers']\n",
        "    start_idxs, end_idxs = [], []\n",
        "\n",
        "    for i,  offset in enumerate(offset_mapping):\n",
        "\n",
        "        ## ans 在原文中的位置\n",
        "        sample_idx = orig_sample_idxs[i] #用overflow_to_sample_mapping 對照回原資料的id\n",
        "        answer = answers[sample_idx]\n",
        "\n",
        "        ans_start_char = answer['answer_start'][0]\n",
        "        ans_end_char = ans_start_char + len(answer['text'][0])\n",
        "\n",
        "\n",
        "        #找到切割後的句子arr中context 位置\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        # print(len(offset), len(sequence_ids))\n",
        "\n",
        "        ctx_start = sequence_ids.index(1) # first occurrence\n",
        "        ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1 # last occurrence\n",
        "\n",
        "        # 找到切割後contex內ans的位置，若不包含ans, 回傳 0,0\n",
        "        new_s, new_e = find_answer_token_idx(ctx_start, ctx_end,\n",
        "                                             ans_start_char, ans_end_char,\n",
        "                                             offset)\n",
        "        start_idxs.append(new_s)\n",
        "        end_idxs.append(new_e)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_idxs\n",
        "    inputs[\"end_positions\"] = end_idxs\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "Z41AtP6yNPu_"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = raw_datasets['train'].map(tokenize_fn_train,\n",
        "                                          batched=True,\n",
        "                                          remove_columns=raw_datasets['train'].column_names)\n",
        "len(raw_datasets['train']), len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhxB4VpdNggD",
        "outputId": "0ab74a75-d10e-489b-8e79-7ce0660ad585"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87599, 88729)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn_val(batch):\n",
        "\n",
        "    questions = [q.strip() for q in batch['question']]\n",
        "    inputs = tokenizer(text = questions,\n",
        "                       text_pair = batch['context'],\n",
        "                       max_length=max_length, # the maximum length of context\n",
        "                       truncation='only_second', # 不能truncate question\n",
        "                       stride=stride, # move forward 50 (overlap 50 chars)\n",
        "                       return_overflowing_tokens=True,\n",
        "                       return_offsets_mapping=True,\n",
        "                       padding='max_length')\n",
        "\n",
        "    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    sample_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = orig_sample_idxs[i]\n",
        "        sample_ids.append(batch['id'][sample_idx]) #存原本資料的 str id\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)] # mask掉question的部分為None\n",
        "\n",
        "    inputs['sample_id'] = sample_ids\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "iZXQvIFDPRjc"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = raw_datasets['validation'].map(tokenize_fn_val,\n",
        "                                          batched=True,\n",
        "                                          remove_columns=raw_datasets['validation'].column_names)\n",
        "len(raw_datasets['validation']), len(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwsYD3BCQqkM",
        "outputId": "a7f80565-9c1c-4f15-c106-97e4d0f73b6c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10570, 10822)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "- model output 是一連串的logits, 需要將model output 對回string"
      ],
      "metadata": {
        "id": "7LLJQX9kQwSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 用內建的metric"
      ],
      "metadata": {
        "id": "NTzEQVxzWM12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"squad\")"
      ],
      "metadata": {
        "id": "Pcr3oIZeWVvs"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## input example & format\n",
        "\n",
        "## 預測結果須包含id, prediction_text，且每筆資料用dict表示\n",
        "predicted_answers = [{'id': '1', 'prediction_text': 'Albert Einstein'},\n",
        "                     {'id': '2', 'prediction_text': 'physicist'},\n",
        "                     {'id': '3', 'prediction_text': 'general relativity'}]\n",
        "\n",
        "## 實際值須包含id, answers，answer內在用dict存ans, ans_start\n",
        "true_answers = [{'id': '1', 'answers': {'text': ['Albert Einstein'], 'answer_start': [100]}},\n",
        "                {'id': '2', 'answers': {'text': ['physicist'], 'answer_start': [100]}},\n",
        "                {'id': '3', 'answers': {'text': ['special relativity'], 'answer_start': [100]}}]\n",
        "\n",
        "metric.compute(predictions=predicted_answers, references=true_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMbIl-X_WVtW",
        "outputId": "254091cf-aaa9-41b0-8945-0eb87852d0b5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 66.66666666666667, 'f1': 83.33333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model output"
      ],
      "metadata": {
        "id": "zr7VhOxiWVqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01 data set\n",
        "small_validation_dataset = raw_datasets[\"validation\"].select(range(100))\n",
        "\n",
        "# 02 tokenizer\n",
        "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "# 03 tokenize data\n",
        "old_tokenizer = tokenizer\n",
        "tokenizer = tokenizer2 #用pretrain tokenizer做\n",
        "\n",
        "small_validation_processed = small_validation_dataset.map(tokenize_fn_val,\n",
        "                                                          batched=True,\n",
        "                                                          remove_columns=raw_datasets[\"validation\"].column_names)\n",
        "tokenizer = old_tokenizer #換回來"
      ],
      "metadata": {
        "id": "jUZoQSr1WVoX"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 04 get the model outputs\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "small_model_inputs = small_validation_processed.remove_columns(['sample_id', 'offset_mapping'])\n",
        "small_model_inputs.set_format('torch')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "small_model_inputs_gpu = {k: small_model_inputs[k].to(device) for k in small_model_inputs.column_names}\n",
        "\n",
        "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = trained_model(**small_model_inputs_gpu)"
      ],
      "metadata": {
        "id": "IBWfV26LXjF_"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 移回cpu 做後續運算\n",
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()"
      ],
      "metadata": {
        "id": "abKKXUzJbG6v"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs\n",
        "## start_logits 跟 end_logits 存ans在每個位置的機率"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcwDWy6LYHyj",
        "outputId": "62ee3be3-ccc7-421a-a6df-d438f5211b78"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -2.2607,  -5.1783,  -5.2709,  ...,  -9.5243,  -9.5183,  -9.5288],\n",
              "        [ -2.5961,  -5.5482,  -5.5313,  ...,  -9.9598,  -9.9533,  -9.9860],\n",
              "        [ -3.7127,  -7.1848,  -8.5388,  ..., -11.6557, -11.6571, -11.6505],\n",
              "        ...,\n",
              "        [ -2.0260,  -4.4167,  -4.4980,  ...,  -8.1479,  -8.1530,  -8.1760],\n",
              "        [ -4.1553,  -5.8304,  -7.1643,  ..., -10.5255, -10.5251, -10.4890],\n",
              "        [ -3.2000,  -5.8162,  -6.7249,  ...,  -9.4935,  -9.5038,  -9.4871]],\n",
              "       device='cuda:0'), end_logits=tensor([[ -0.7353,  -4.9236,  -5.1048,  ...,  -8.8734,  -8.8916,  -8.8550],\n",
              "        [ -1.3056,  -5.3870,  -5.4945,  ...,  -9.4895,  -9.5039,  -9.4959],\n",
              "        [ -2.7649,  -7.2201,  -9.0916,  ..., -11.3106, -11.3414, -11.2702],\n",
              "        ...,\n",
              "        [ -0.0768,  -4.8210,  -4.4374,  ...,  -8.0483,  -8.0502,  -7.9903],\n",
              "        [ -2.7347,  -5.3650,  -7.2549,  ..., -10.0498, -10.0661,  -9.9886],\n",
              "        [ -1.0991,  -4.2569,  -6.1267,  ...,  -8.6882,  -8.6889,  -8.6272]],\n",
              "       device='cuda:0'), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_validation_processed['sample_id'][:5] ## 每筆資料獨立的id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgManHRma-oR",
        "outputId": "89fa07dd-c4a2-4f12-dab3-7dc19da0d237"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['56be4db0acb8001400a502ec',\n",
              " '56be4db0acb8001400a502ed',\n",
              " '56be4db0acb8001400a502ee',\n",
              " '56be4db0acb8001400a502ef',\n",
              " '56be4db0acb8001400a502f0']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### context window會的做法一筆資料會expand成多筆，需要用mapping 對回原本的關係\n",
        "->  example: {'56be4db0acb8001400a502ec': [0, 1, 2, 3], ...}\n"
      ],
      "metadata": {
        "id": "NkAjHEekbOaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "sample_id2idxs = defaultdict(list)\n",
        "for i, sample_id in enumerate(small_validation_processed['sample_id']):\n",
        "    sample_id2idxs[sample_id].append(i)"
      ],
      "metadata": {
        "id": "f-jq-OEybds-"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 取 start_logits, end_logits機率最大的當作答案，並將位置對回文字"
      ],
      "metadata": {
        "id": "AYHR-HACb3WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_largest = 20 #只看前20的機率(不然會算很久)\n",
        "max_answer_length = 30\n",
        "predicted_answers = []\n",
        "\n",
        "for sample in small_validation_dataset: ## 原資料集\n",
        "    sample_id = sample['id']\n",
        "    context = sample['context']\n",
        "\n",
        "    ## 最佳分數 ＆ans\n",
        "    best_score = float('-inf')\n",
        "    best_answer = None\n",
        "\n",
        "    for idx in sample_id2idxs[sample_id]: ## 用上一步的mapping去找expand後的資料\n",
        "        start_logit = start_logits[idx] # (384,) vector\n",
        "        end_logit = end_logits[idx] # (384,) vector\n",
        "        offsets = small_validation_processed[idx]['offset_mapping']\n",
        "\n",
        "        ## 取P(start)*P(end)最大的 -> log(start) + log(end) 最大\n",
        "        start_indices = (-start_logit).argsort() #descending order\n",
        "        end_indices = (-end_logit).argsort()\n",
        "\n",
        "        for start_idx in start_indices[:n_largest]:\n",
        "            for end_idx in end_indices[:n_largest]:\n",
        "                ## 先確認是否有不合法的組合\n",
        "                if offsets[start_idx] is None or offsets[end_idx] is None or end_idx < start_idx or end_idx - start_idx + 1 > max_answer_length:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                # 計算分數並更新\n",
        "                score = start_logit[start_idx] + end_logit[end_idx]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "\n",
        "                    ## 取得ans 字串\n",
        "                    first_ch = offsets[start_idx][0]\n",
        "                    last_ch = offsets[end_idx][1]\n",
        "                    best_answer = context[first_ch:last_ch]\n",
        "\n",
        "    # save best answer\n",
        "    predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})"
      ],
      "metadata": {
        "id": "mNxbISUZdQCS"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_answers[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_6qKVhYf6HG",
        "outputId": "7418710b-20e7-402b-d0d2-78f725a10913"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_validation_dataset['answers'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgC3AFrOhbwc",
        "outputId": "1e132d0f-2880-4be3-9958-f7933d1a897c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
              " 'answer_start': [177, 177, 177]}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## true answer 格式\n",
        "true_answers = [{'id': x['id'], 'answers': x['answers']} for x in small_validation_dataset]\n",
        "\n",
        "## 計算\n",
        "metric.compute(predictions=predicted_answers, references=true_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRZxrwz_gxLc",
        "outputId": "7473ce6f-6d3a-4d13-fbc1-3f371a7dae3d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 83.0, 'f1': 88.25000000000004}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics function"
      ],
      "metadata": {
        "id": "WQJ8-i5Dhi0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\n",
        "    ## 建立 sample to index 對照表\n",
        "    sample_id2idxs = defaultdict(list)\n",
        "    for i, sample_id in enumerate(processed_dataset['sample_id']):\n",
        "        sample_id2idxs[sample_id].append(i)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for sample in orig_dataset: ## 原資料集\n",
        "        sample_id = sample['id']\n",
        "        context = sample['context']\n",
        "\n",
        "        ## 最佳分數 ＆ans\n",
        "        best_score = float('-inf')\n",
        "        best_answer = None\n",
        "        for idx in sample_id2idxs[sample_id]: ## 用上一步的mapping去找expand後的資料\n",
        "            start_logit = start_logits[idx] # (384,) vector\n",
        "            end_logit = end_logits[idx] # (384,) vector\n",
        "            offsets = processed_dataset[idx]['offset_mapping']\n",
        "\n",
        "            ## 取P(start)*P(end)最大的 -> log(start) + log(end) 最大\n",
        "            start_indices = (-start_logit).argsort() #descending order\n",
        "            end_indices = (-end_logit).argsort()\n",
        "\n",
        "            for start_idx in start_indices[:n_largest]:\n",
        "                for end_idx in end_indices[:n_largest]:\n",
        "                    ## 先確認是否有不合法的組合\n",
        "                    if offsets[start_idx] is None or offsets[end_idx] is None or end_idx < start_idx or end_idx - start_idx + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    # 計算分數並更新\n",
        "                    score = start_logit[start_idx] + end_logit[end_idx]\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "\n",
        "                        ## 取得ans 字串\n",
        "                        first_ch = offsets[start_idx][0]\n",
        "                        last_ch = offsets[end_idx][1]\n",
        "                        best_answer = context[first_ch:last_ch]\n",
        "\n",
        "        # save best answer\n",
        "        predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})\n",
        "\n",
        "    ## true answer 格式\n",
        "    true_answers = [{'id': x['id'], 'answers': x['answers']} for x in orig_dataset]\n",
        "\n",
        "    ## 計算\n",
        "    return metric.compute(predictions=predicted_answers, references=true_answers)"
      ],
      "metadata": {
        "id": "qmcLhuIYyM79"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run our function on the same mini dataset as before\n",
        "compute_metrics(start_logits,\n",
        "                end_logits,\n",
        "                small_validation_processed,\n",
        "                small_validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw2ErxlvzEt7",
        "outputId": "a02bf268-5ccf-4b50-8ce9-43ad20926e43"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 83.0, 'f1': 88.25000000000004}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate\n",
        "- 用trainer 訓練model"
      ],
      "metadata": {
        "id": "Wd83YgrnzJkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments('finetuned-squad',\n",
        "                         evaluation_strategy='no',\n",
        "                         save_strategy='epoch',\n",
        "                         learning_rate=2e-5,\n",
        "                         num_train_epochs=3,\n",
        "                         weight_decay=0.01,\n",
        "                         fp16=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO1b-Qn4zySz",
        "outputId": "478dbb1c-e2a3-432f-d42e-09e515043533"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  args=args,\n",
        "                  train_dataset=train_dataset,\n",
        "                #   train_dataset=train_dataset.shuffle(seed=42).select(range(1_000)),\n",
        "                  eval_dataset=val_dataset,\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_kRYF0_IzyQW",
        "outputId": "814cbc8f-c921-4756-e18c-ce916bc4bdba"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33276' max='33276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33276/33276 1:08:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.225200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.980600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.763800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.642300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.479600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.441500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.390700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.367800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.334400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.373200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.305900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.286500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>1.297900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>1.269900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.193800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>1.203500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.188400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.985800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.951900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.954800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.941600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.944500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.936300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.928300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.952500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.906600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.929400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.923500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.888600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.929600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.867700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.899300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.897600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.940200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.880800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.862000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.789500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.707300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.701500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.675200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.697300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.687400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.685200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.664200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.683500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.658400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.667200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.643700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.671200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.688000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=33276, training_loss=1.045403349498442, metrics={'train_runtime': 4106.54, 'train_samples_per_second': 64.82, 'train_steps_per_second': 8.103, 'total_flos': 2.608361755366349e+16, 'train_loss': 1.045403349498442, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_output = trainer.predict(val_dataset)\n",
        "trainer_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "JZNE-ssbzyNv",
        "outputId": "bb5e3e9c-33f9-40ff-98da-e1e5897200de"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=(array([[ -9.3046875, -10.84375  , -11.2421875, ..., -11.6875   ,\n",
              "        -11.6953125, -11.7109375],\n",
              "       [ -9.34375  , -10.9296875, -11.3046875, ..., -11.703125 ,\n",
              "        -11.703125 , -11.7265625],\n",
              "       [ -9.203125 ,  -9.453125 , -10.828125 , ..., -11.71875  ,\n",
              "        -11.6640625, -11.734375 ],\n",
              "       ...,\n",
              "       [ -3.3535156, -10.       , -11.5546875, ..., -11.796875 ,\n",
              "        -11.765625 , -11.75     ],\n",
              "       [ -3.4863281, -11.3203125, -11.1875   , ..., -11.75     ,\n",
              "        -11.59375  , -11.7265625],\n",
              "       [ -1.3916016, -10.7265625, -11.28125  , ..., -11.8046875,\n",
              "        -11.7734375, -11.734375 ]], dtype=float32), array([[ -8.1875    , -11.03125   , -10.9296875 , ..., -11.515625  ,\n",
              "        -11.5234375 , -11.4921875 ],\n",
              "       [ -8.1953125 , -10.953125  , -10.8515625 , ..., -11.5078125 ,\n",
              "        -11.5078125 , -11.4765625 ],\n",
              "       [ -8.0390625 ,  -8.8359375 , -11.34375   , ..., -11.5       ,\n",
              "        -11.546875  , -11.46875   ],\n",
              "       ...,\n",
              "       [ -2.3730469 , -10.3046875 , -11.2734375 , ..., -11.4375    ,\n",
              "        -11.4921875 , -11.5       ],\n",
              "       [ -2.6914062 , -11.046875  , -10.84375   , ..., -11.4375    ,\n",
              "        -11.5546875 , -11.46875   ],\n",
              "       [ -0.43286133, -11.1015625 , -11.515625  , ..., -11.4453125 ,\n",
              "        -11.484375  , -11.5234375 ]], dtype=float32)), label_ids=None, metrics={'test_runtime': 48.1729, 'test_samples_per_second': 224.649, 'test_steps_per_second': 28.086})"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, _, _ = trainer_output\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIwEuKgozyLN",
        "outputId": "08f7d904-97b7-4a0d-960e-1f5a6e3f5643"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ -9.3046875, -10.84375  , -11.2421875, ..., -11.6875   ,\n",
              "         -11.6953125, -11.7109375],\n",
              "        [ -9.34375  , -10.9296875, -11.3046875, ..., -11.703125 ,\n",
              "         -11.703125 , -11.7265625],\n",
              "        [ -9.203125 ,  -9.453125 , -10.828125 , ..., -11.71875  ,\n",
              "         -11.6640625, -11.734375 ],\n",
              "        ...,\n",
              "        [ -3.3535156, -10.       , -11.5546875, ..., -11.796875 ,\n",
              "         -11.765625 , -11.75     ],\n",
              "        [ -3.4863281, -11.3203125, -11.1875   , ..., -11.75     ,\n",
              "         -11.59375  , -11.7265625],\n",
              "        [ -1.3916016, -10.7265625, -11.28125  , ..., -11.8046875,\n",
              "         -11.7734375, -11.734375 ]], dtype=float32),\n",
              " array([[ -8.1875    , -11.03125   , -10.9296875 , ..., -11.515625  ,\n",
              "         -11.5234375 , -11.4921875 ],\n",
              "        [ -8.1953125 , -10.953125  , -10.8515625 , ..., -11.5078125 ,\n",
              "         -11.5078125 , -11.4765625 ],\n",
              "        [ -8.0390625 ,  -8.8359375 , -11.34375   , ..., -11.5       ,\n",
              "         -11.546875  , -11.46875   ],\n",
              "        ...,\n",
              "        [ -2.3730469 , -10.3046875 , -11.2734375 , ..., -11.4375    ,\n",
              "         -11.4921875 , -11.5       ],\n",
              "        [ -2.6914062 , -11.046875  , -10.84375   , ..., -11.4375    ,\n",
              "         -11.5546875 , -11.46875   ],\n",
              "        [ -0.43286133, -11.1015625 , -11.515625  , ..., -11.4453125 ,\n",
              "         -11.484375  , -11.5234375 ]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(start_logits), len(end_logits), len(val_dataset), len(raw_datasets[\"validation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjLKTgls2-mo",
        "outputId": "ee200edb-7538-4ccd-f2de-1a15df1653c2"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10822, 10822, 10822, 10570)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits, end_logits = predictions"
      ],
      "metadata": {
        "id": "RcrJGVYO2W_n"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_metrics(start_logits,\n",
        "                end_logits,\n",
        "                val_dataset, # processed\n",
        "                raw_datasets[\"validation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2O5n98J2Zw-",
        "outputId": "4cc52c44-3256-4083-d32c-a3f45a46130a"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 76.87795648060549, 'f1': 85.17484460092126}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model & predict"
      ],
      "metadata": {
        "id": "VtAABFPp3PvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "trainer.save_model('my_qa_model')\n",
        "\n",
        "qa = pipeline('question-answering',\n",
        "              model='my_qa_model',\n",
        "              device=0)"
      ],
      "metadata": {
        "id": "dIzjgyqQ3rw6"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Today I went to the store to purchase a carton of milk.\"\n",
        "question = \"What did I buy?\"\n",
        "\n",
        "qa(context=context, question=question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui8rlPJO2qBy",
        "outputId": "07a1ba4d-5162-4dae-9f47-ce7f7c99416b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.5982309579849243,\n",
              " 'start': 38,\n",
              " 'end': 54,\n",
              " 'answer': 'a carton of milk'}"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMGo1w5r32vG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}